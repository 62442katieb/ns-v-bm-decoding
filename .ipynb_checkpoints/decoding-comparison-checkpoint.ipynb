{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Read in each NS decoding csv to a pandas dataframe<br>\n",
    "2. Read in list of NS terms associated with each BD and PC as dictionary<br>\n",
    "3. From (1), extract r values corresponding to each significant BD/PC using (2)<br>\n",
    "4. Return the significant BM terms that are <br>\n",
    "    a. Represented by NS terms with (+) correlations<br>\n",
    "    b. Represented by NS terms with (-) correlations<br>\n",
    "    c. Not represented by NS terms<br>\n",
    "5. Return the positive NS terms that are<br>\n",
    "    a. Not significant in BM<br>\n",
    "    b. Significant in BM<br>\n",
    "    c. Not in BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "rois = ['fusiform', 'paracentral', 'pcc', 'orbitofrontal', \n",
    "        'precuneus', 'temporal', 'uncus', 'visual', 'acc',\n",
    "        '']\n",
    "decode = {}\n",
    "data_dir = '/Users/Katie/Dropbox/Data/NSvBM-decoding/decoded'\n",
    "\n",
    "for i in rois:\n",
    "    ns_decoding = np.genfromtxt('{0}/NS/decoded_{1}.txt'.format(data_dir, i), delimiter=',')\n",
    "    ns_decoding = ns_decoding[1:,:]\n",
    "    decode[i] = ns_decoding[:,1]\n",
    "    \n",
    "df = pd.read_csv('{0}/NS/decoded_uncus.txt'.format(data_dir), sep=',', index_col=0)\n",
    "index = df.index.values.tolist()\n",
    "neurosynth_decoding = pd.DataFrame(decode, index=index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/Katie/Dropbox/Data/NSvBM-decoding/decoded'\n",
    "neurosynth_decoding = pd.read_csv('{0}/NS/all.csv'.format(data_dir), index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fusiform</th>\n",
       "      <th>orbitofrontal</th>\n",
       "      <th>paracentral</th>\n",
       "      <th>pcc</th>\n",
       "      <th>precuneus</th>\n",
       "      <th>temporal</th>\n",
       "      <th>uncus</th>\n",
       "      <th>visual</th>\n",
       "      <th>acc</th>\n",
       "      <th>thalamus</th>\n",
       "      <th>mt</th>\n",
       "      <th>hypothalamus</th>\n",
       "      <th>angular</th>\n",
       "      <th>dlpfc</th>\n",
       "      <th>cerebellum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1550</td>\n",
       "      <td>-0.0227</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>-0.0386</td>\n",
       "      <td>-0.0663</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>-0.2110</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>-0.1458</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>0.1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1315</td>\n",
       "      <td>-0.0711</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>-0.0289</td>\n",
       "      <td>-0.1123</td>\n",
       "      <td>-0.0856</td>\n",
       "      <td>-0.0521</td>\n",
       "      <td>-0.1077</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>-0.1755</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>-0.0697</td>\n",
       "      <td>-0.0815</td>\n",
       "      <td>0.0313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1594</td>\n",
       "      <td>-0.0584</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>-0.0891</td>\n",
       "      <td>-0.2117</td>\n",
       "      <td>-0.0193</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>-0.1753</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>-0.2886</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>-0.2224</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>0.1712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.2374</td>\n",
       "      <td>0.1512</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>-0.0063</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>-0.1066</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>-0.2550</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>-0.0298</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>-0.0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1152</td>\n",
       "      <td>-0.0693</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>-0.0346</td>\n",
       "      <td>-0.1396</td>\n",
       "      <td>-0.0537</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>-0.1048</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>-0.1731</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>-0.1718</td>\n",
       "      <td>-0.1331</td>\n",
       "      <td>0.0606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1239</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>-0.0366</td>\n",
       "      <td>-0.0750</td>\n",
       "      <td>-0.0868</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>-0.0912</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>-0.1344</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>-0.1212</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>0.0319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1416</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>-0.0173</td>\n",
       "      <td>-0.1091</td>\n",
       "      <td>-0.0137</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>-0.1357</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>-0.1787</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>-0.0813</td>\n",
       "      <td>0.1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0313</td>\n",
       "      <td>-0.0141</td>\n",
       "      <td>-0.0523</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0972</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>-0.0575</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>-0.0750</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>-0.0831</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>0.0806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1521</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>-0.0493</td>\n",
       "      <td>-0.1272</td>\n",
       "      <td>-0.0162</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>-0.1286</td>\n",
       "      <td>0.0617</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>-0.1622</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>-0.1157</td>\n",
       "      <td>-0.0907</td>\n",
       "      <td>-0.0058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1252</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>0.0559</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>-0.0094</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>-0.1161</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>-0.1833</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>-0.0669</td>\n",
       "      <td>-0.0827</td>\n",
       "      <td>0.0918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0914</td>\n",
       "      <td>-0.0263</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0893</td>\n",
       "      <td>-0.1256</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>-0.0494</td>\n",
       "      <td>-0.1205</td>\n",
       "      <td>0.0703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1752</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>-0.0513</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.0559</td>\n",
       "      <td>-0.1064</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>-0.0851</td>\n",
       "      <td>-0.2238</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>-0.0372</td>\n",
       "      <td>-0.0942</td>\n",
       "      <td>-0.0594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0320</td>\n",
       "      <td>-0.0212</td>\n",
       "      <td>-0.0467</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>-0.1059</td>\n",
       "      <td>-0.0227</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>-0.0260</td>\n",
       "      <td>-0.1228</td>\n",
       "      <td>-0.0627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0911</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>-0.0445</td>\n",
       "      <td>-0.0250</td>\n",
       "      <td>-0.1205</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>-0.0639</td>\n",
       "      <td>0.0907</td>\n",
       "      <td>-0.0290</td>\n",
       "      <td>-0.1600</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>-0.1547</td>\n",
       "      <td>-0.1812</td>\n",
       "      <td>0.1410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1244</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>-0.0717</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>-0.1312</td>\n",
       "      <td>0.1795</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>-0.1472</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>-0.2061</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>-0.0943</td>\n",
       "      <td>-0.1233</td>\n",
       "      <td>0.0134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1509</td>\n",
       "      <td>-0.0346</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>-0.1131</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>-0.0421</td>\n",
       "      <td>-0.1913</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>-0.0821</td>\n",
       "      <td>-0.1468</td>\n",
       "      <td>-0.0357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0912</td>\n",
       "      <td>-0.0493</td>\n",
       "      <td>-0.0368</td>\n",
       "      <td>-0.0107</td>\n",
       "      <td>-0.1519</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>-0.0888</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>-0.1383</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>-0.1337</td>\n",
       "      <td>-0.1529</td>\n",
       "      <td>0.1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>-0.0551</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>-0.0656</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>-0.1538</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>-0.0888</td>\n",
       "      <td>-0.0517</td>\n",
       "      <td>-0.0139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1544</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>-0.0441</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>-0.1031</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>-0.1392</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>-0.0091</td>\n",
       "      <td>-0.2191</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>-0.0666</td>\n",
       "      <td>-0.1492</td>\n",
       "      <td>-0.0423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1192</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>-0.0224</td>\n",
       "      <td>-0.0889</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>-0.1298</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>-0.1663</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>-0.0950</td>\n",
       "      <td>-0.1319</td>\n",
       "      <td>0.0861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1401</td>\n",
       "      <td>-0.0653</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>-0.0186</td>\n",
       "      <td>-0.1063</td>\n",
       "      <td>-0.0536</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>-0.1229</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>-0.2116</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>-0.1053</td>\n",
       "      <td>-0.1477</td>\n",
       "      <td>0.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0996</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.0231</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>-0.0929</td>\n",
       "      <td>0.1844</td>\n",
       "      <td>0.1639</td>\n",
       "      <td>-0.1161</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>-0.0325</td>\n",
       "      <td>-0.1540</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>-0.0524</td>\n",
       "      <td>-0.1074</td>\n",
       "      <td>-0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0884</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>-0.0308</td>\n",
       "      <td>-0.1433</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>-0.0803</td>\n",
       "      <td>-0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1363</td>\n",
       "      <td>-0.0751</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>-0.0903</td>\n",
       "      <td>-0.1730</td>\n",
       "      <td>-0.1515</td>\n",
       "      <td>-0.0829</td>\n",
       "      <td>-0.0961</td>\n",
       "      <td>-0.0238</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.1565</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>-0.1711</td>\n",
       "      <td>-0.1773</td>\n",
       "      <td>0.1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1333</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0349</td>\n",
       "      <td>-0.0460</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.0956</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>-0.1540</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>-0.0571</td>\n",
       "      <td>-0.0515</td>\n",
       "      <td>0.0278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1910</td>\n",
       "      <td>0.0629</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>-0.0996</td>\n",
       "      <td>0.2242</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>-0.0826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0784</td>\n",
       "      <td>-0.0180</td>\n",
       "      <td>-0.0053</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>-0.0213</td>\n",
       "      <td>-0.0266</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>-0.0310</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>-0.1142</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>-0.0615</td>\n",
       "      <td>-0.0570</td>\n",
       "      <td>-0.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1116</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>-0.0540</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>-0.0679</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>-0.0063</td>\n",
       "      <td>-0.1577</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>-0.0462</td>\n",
       "      <td>-0.0741</td>\n",
       "      <td>-0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0591</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.0732</td>\n",
       "      <td>-0.0261</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.0853</td>\n",
       "      <td>-0.0799</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>-0.1152</td>\n",
       "      <td>0.1087</td>\n",
       "      <td>-0.0194</td>\n",
       "      <td>-0.0383</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0591</td>\n",
       "      <td>-0.0473</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>-0.0168</td>\n",
       "      <td>-0.0765</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>-0.0689</td>\n",
       "      <td>-0.0682</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>-0.0823</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>-0.0398</td>\n",
       "      <td>-0.0686</td>\n",
       "      <td>0.0267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiley</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>-0.0623</td>\n",
       "      <td>-0.0107</td>\n",
       "      <td>-0.0183</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>-0.0132</td>\n",
       "      <td>-0.1049</td>\n",
       "      <td>-0.0157</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>-0.1048</td>\n",
       "      <td>0.0425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiley periodicals</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>-0.0748</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>-0.0305</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>-0.0244</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>-0.0210</td>\n",
       "      <td>-0.0912</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>-0.0364</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>-0.1070</td>\n",
       "      <td>0.0583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wise</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1603</td>\n",
       "      <td>-0.0850</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>-0.0340</td>\n",
       "      <td>-0.1577</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>-0.1139</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>-0.0950</td>\n",
       "      <td>-0.2049</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>-0.1634</td>\n",
       "      <td>-0.2621</td>\n",
       "      <td>-0.0338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wm</th>\n",
       "      <td>Functional</td>\n",
       "      <td>-0.0051</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>-0.0081</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>-0.2238</td>\n",
       "      <td>-0.2011</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>-0.1046</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>0.3086</td>\n",
       "      <td>-0.0516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wm task</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0662</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.1736</td>\n",
       "      <td>-0.1292</td>\n",
       "      <td>-0.1199</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>-0.0917</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>-0.0190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>women</th>\n",
       "      <td>Subject-related</td>\n",
       "      <td>-0.0790</td>\n",
       "      <td>0.0920</td>\n",
       "      <td>-0.0348</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.1230</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>-0.1094</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.1119</td>\n",
       "      <td>-0.1286</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>-0.1358</td>\n",
       "      <td>-0.0708</td>\n",
       "      <td>0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.3193</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>-0.2091</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>-0.1460</td>\n",
       "      <td>0.1639</td>\n",
       "      <td>-0.1640</td>\n",
       "      <td>-0.0926</td>\n",
       "      <td>0.3521</td>\n",
       "      <td>-0.2327</td>\n",
       "      <td>0.2701</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>-0.1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word form</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.4030</td>\n",
       "      <td>-0.0731</td>\n",
       "      <td>-0.1724</td>\n",
       "      <td>-0.0654</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.0874</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>-0.2237</td>\n",
       "      <td>-0.1028</td>\n",
       "      <td>0.4285</td>\n",
       "      <td>-0.1535</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word pairs</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.1502</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>-0.0317</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>-0.0599</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>-0.0883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word recognition</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>-0.0358</td>\n",
       "      <td>-0.0700</td>\n",
       "      <td>-0.0264</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>-0.0686</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>-0.1074</td>\n",
       "      <td>-0.1221</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>-0.1292</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>-0.0647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.3088</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>-0.1607</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>-0.0413</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>-0.1465</td>\n",
       "      <td>-0.1345</td>\n",
       "      <td>0.2815</td>\n",
       "      <td>-0.1691</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.1352</td>\n",
       "      <td>-0.0973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>Functional</td>\n",
       "      <td>-0.1538</td>\n",
       "      <td>-0.0132</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>-0.0612</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>-0.0398</td>\n",
       "      <td>-0.0817</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>-0.1506</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>-0.0588</td>\n",
       "      <td>-0.0735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>working</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.1531</td>\n",
       "      <td>-0.0855</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>-0.2408</td>\n",
       "      <td>-0.3057</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>-0.0645</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>-0.2384</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>-0.0222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>working memory</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.1536</td>\n",
       "      <td>-0.0838</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>-0.2493</td>\n",
       "      <td>-0.3093</td>\n",
       "      <td>0.0866</td>\n",
       "      <td>-0.0645</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>-0.2422</td>\n",
       "      <td>0.2379</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>-0.0227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>0.1176</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.0967</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>-0.1174</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>-0.0217</td>\n",
       "      <td>0.1148</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worse</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0793</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>-0.0613</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>-0.0435</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>-0.0837</td>\n",
       "      <td>-0.1033</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.0856</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>-0.0608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>written</th>\n",
       "      <td>Functional</td>\n",
       "      <td>0.2617</td>\n",
       "      <td>-0.0413</td>\n",
       "      <td>-0.1133</td>\n",
       "      <td>-0.0446</td>\n",
       "      <td>-0.0119</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>-0.0457</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>-0.1532</td>\n",
       "      <td>-0.0798</td>\n",
       "      <td>0.2781</td>\n",
       "      <td>-0.0941</td>\n",
       "      <td>0.0818</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>-0.0158</td>\n",
       "      <td>-0.0617</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.1416</td>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>-0.1237</td>\n",
       "      <td>-0.0044</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-0.1153</td>\n",
       "      <td>-0.0578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>-0.0341</td>\n",
       "      <td>-0.0378</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>-0.0736</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>-0.0071</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>-0.0375</td>\n",
       "      <td>-0.0728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1216</td>\n",
       "      <td>-0.0554</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>-0.0401</td>\n",
       "      <td>-0.1131</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>-0.0881</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>-0.0462</td>\n",
       "      <td>-0.2076</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>-0.1276</td>\n",
       "      <td>-0.2056</td>\n",
       "      <td>0.0679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yield</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.0452</td>\n",
       "      <td>-0.0102</td>\n",
       "      <td>-0.0424</td>\n",
       "      <td>-0.0314</td>\n",
       "      <td>-0.0882</td>\n",
       "      <td>-0.0224</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>-0.1015</td>\n",
       "      <td>-0.0320</td>\n",
       "      <td>-0.0249</td>\n",
       "      <td>-0.0516</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>-0.0622</td>\n",
       "      <td>-0.0840</td>\n",
       "      <td>0.0171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yielded</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>-0.1284</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>-0.0506</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>-0.0509</td>\n",
       "      <td>-0.0649</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>-0.1612</td>\n",
       "      <td>-0.0081</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>-0.0751</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>Subject-related</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>-0.0408</td>\n",
       "      <td>-0.1450</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>0.0366</td>\n",
       "      <td>-0.0853</td>\n",
       "      <td>0.1516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young adults</th>\n",
       "      <td>Subject-related</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>-0.0163</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.0160</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>-0.0375</td>\n",
       "      <td>-0.1365</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>-0.0776</td>\n",
       "      <td>0.0116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young healthy</th>\n",
       "      <td>Subject-related</td>\n",
       "      <td>-0.0160</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0259</td>\n",
       "      <td>-0.0278</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>-0.0570</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0786</td>\n",
       "      <td>-0.1285</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>-0.0318</td>\n",
       "      <td>-0.0115</td>\n",
       "      <td>0.2458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger</th>\n",
       "      <td>Subject-related</td>\n",
       "      <td>0.0599</td>\n",
       "      <td>0.0677</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>-0.0695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger adults</th>\n",
       "      <td>Subject-related</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1529</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>-0.0442</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.0255</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>-0.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>-0.1005</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>-0.1109</td>\n",
       "      <td>-0.0903</td>\n",
       "      <td>-0.1201</td>\n",
       "      <td>-0.1155</td>\n",
       "      <td>-0.0737</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>-0.1108</td>\n",
       "      <td>-0.0644</td>\n",
       "      <td>-0.0614</td>\n",
       "      <td>-0.0091</td>\n",
       "      <td>0.0915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FALSE</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>-0.0156</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.2323</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>-0.0107</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.2392</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>-0.1346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRUE</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>-0.0210</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0723</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0826</td>\n",
       "      <td>-0.0164</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.1778</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>-0.0801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3169 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              type  fusiform  orbitofrontal  paracentral  \\\n",
       "feature                                                                    \n",
       "1                      Non-content   -0.1550        -0.0227       0.0905   \n",
       "1                      Non-content   -0.1315        -0.0711       0.0654   \n",
       "5                      Non-content   -0.1594        -0.0584       0.0291   \n",
       "10                     Non-content   -0.2374         0.1512       0.0269   \n",
       "11                     Non-content   -0.1152        -0.0693       0.0224   \n",
       "12                     Non-content   -0.1239         0.0435       0.1124   \n",
       "13                     Non-content   -0.1416         0.0128       0.0150   \n",
       "14                     Non-content   -0.0313        -0.0141      -0.0523   \n",
       "15                     Non-content   -0.1521        -0.0258       0.0283   \n",
       "16                     Non-content   -0.1252         0.0514       0.0559   \n",
       "17                     Non-content   -0.0914        -0.0263       0.0634   \n",
       "18                     Non-content   -0.1752         0.0306      -0.0226   \n",
       "19                     Non-content   -0.0320        -0.0212      -0.0467   \n",
       "20                     Non-content   -0.0911         0.0073      -0.0445   \n",
       "21                     Non-content   -0.1244         0.0576      -0.0717   \n",
       "22                     Non-content   -0.1509        -0.0346      -0.0408   \n",
       "23                     Non-content   -0.0912        -0.0493      -0.0368   \n",
       "24                     Non-content   -0.1224        -0.0247       0.0347   \n",
       "25                     Non-content   -0.1544         0.0565      -0.0441   \n",
       "26                     Non-content   -0.1192        -0.0003       0.0597   \n",
       "27                     Non-content   -0.1401        -0.0653       0.0529   \n",
       "28                     Non-content   -0.0996         0.0489      -0.0231   \n",
       "29                     Non-content   -0.0884         0.0478       0.0844   \n",
       "30                     Non-content   -0.1363        -0.0751       0.0678   \n",
       "31                     Non-content   -0.1333         0.0073       0.0553   \n",
       "32                     Non-content   -0.1910         0.0629       0.0906   \n",
       "33                     Non-content   -0.0784        -0.0180      -0.0053   \n",
       "34                     Non-content   -0.1116         0.0481       0.0707   \n",
       "35                     Non-content   -0.0591         0.0055       0.1185   \n",
       "36                     Non-content   -0.0591        -0.0473       0.0328   \n",
       "...                            ...       ...            ...          ...   \n",
       "wiley                  Non-content    0.0295        -0.0623      -0.0107   \n",
       "wiley periodicals      Non-content    0.0265        -0.0748      -0.0148   \n",
       "wise                   Non-content   -0.1603        -0.0850       0.0518   \n",
       "wm                      Functional   -0.0051         0.0327      -0.0081   \n",
       "wm task                 Functional    0.0510         0.0662       0.0241   \n",
       "women              Subject-related   -0.0790         0.0920      -0.0348   \n",
       "word                    Functional    0.3193         0.0477      -0.2091   \n",
       "word form               Functional    0.4030        -0.0731      -0.1724   \n",
       "word pairs              Functional    0.0939         0.0937       0.0061   \n",
       "word recognition        Functional    0.1871        -0.0358      -0.0700   \n",
       "words                   Functional    0.3088         0.0173      -0.1607   \n",
       "work                    Functional   -0.1538        -0.0132       0.0236   \n",
       "working                 Functional    0.0752         0.1531      -0.0855   \n",
       "working memory          Functional    0.0765         0.1536      -0.0838   \n",
       "world                  Non-content    0.0348        -0.0072      -0.0052   \n",
       "worse                  Non-content   -0.0793         0.0735      -0.0613   \n",
       "written                 Functional    0.2617        -0.0413      -0.1133   \n",
       "year                   Non-content    0.0360        -0.0158      -0.0617   \n",
       "year old               Non-content    0.0507        -0.0341      -0.0378   \n",
       "years                  Non-content   -0.1216        -0.0554       0.0323   \n",
       "yield                  Non-content   -0.0452        -0.0102      -0.0424   \n",
       "yielded                Non-content    0.1907        -0.1284       0.0294   \n",
       "young              Subject-related    0.0278         0.0284      -0.0016   \n",
       "young adults       Subject-related   -0.0412         0.0297      -0.0031   \n",
       "young healthy      Subject-related   -0.0160         0.0111       0.0249   \n",
       "younger            Subject-related    0.0599         0.0677       0.0903   \n",
       "younger adults     Subject-related    0.0031         0.0619       0.0742   \n",
       "zone                   Non-content   -0.1005        -0.0572       0.0582   \n",
       "FALSE                  Non-content    0.0124         0.1599      -0.0156   \n",
       "TRUE                   Non-content    0.0168         0.0528      -0.0210   \n",
       "\n",
       "                      pcc  precuneus  temporal   uncus  visual     acc  \\\n",
       "feature                                                                  \n",
       "1                  0.0406    -0.0386   -0.0663  0.0143 -0.1075  0.0517   \n",
       "1                 -0.0289    -0.1123   -0.0856 -0.0521 -0.1077  0.0464   \n",
       "5                 -0.0891    -0.2117   -0.0193  0.0673 -0.1753  0.0318   \n",
       "10                 0.0770    -0.0063    0.0266  0.0071 -0.1066  0.2261   \n",
       "11                -0.0346    -0.1396   -0.0537  0.0276 -0.1048  0.0295   \n",
       "12                -0.0366    -0.0750   -0.0868 -0.0377 -0.0912  0.0401   \n",
       "13                -0.0173    -0.1091   -0.0137  0.0353 -0.1357  0.0761   \n",
       "14                -0.0026    -0.0972   -0.0027  0.0830 -0.0575  0.0426   \n",
       "15                -0.0493    -0.1272   -0.0162  0.0483 -0.1286  0.0617   \n",
       "16                 0.0503    -0.0094    0.0622  0.1499 -0.1161  0.1219   \n",
       "17                 0.0434     0.0252    0.0089  0.0085  0.0090  0.0000   \n",
       "18                 0.0246    -0.0513    0.0542  0.0559 -0.1064  0.0960   \n",
       "19                 0.0226    -0.0057    0.0196  0.0482  0.0053  0.0494   \n",
       "20                -0.0250    -0.1205    0.0170  0.1121 -0.0639  0.0907   \n",
       "21                 0.0051    -0.1312    0.1795  0.1684 -0.1472  0.1403   \n",
       "22                 0.0087    -0.1112    0.0679  0.1089 -0.1131  0.0598   \n",
       "23                -0.0107    -0.1519    0.0064  0.0475 -0.0888  0.0458   \n",
       "24                 0.0065    -0.0551    0.0497  0.0991 -0.0656  0.1059   \n",
       "25                -0.0300    -0.1031    0.0616  0.1498 -0.1392  0.1187   \n",
       "26                -0.0224    -0.0889    0.0525  0.0767 -0.1298  0.0642   \n",
       "27                -0.0186    -0.1063   -0.0536 -0.0103 -0.1229  0.0161   \n",
       "28                 0.0469    -0.0929    0.1844  0.1639 -0.1161  0.1439   \n",
       "29                 0.1100     0.0134    0.1210  0.1139  0.0138  0.1740   \n",
       "30                -0.0903    -0.1730   -0.1515 -0.0829 -0.0961 -0.0238   \n",
       "31                 0.0167    -0.0349   -0.0460  0.0100 -0.0956  0.0690   \n",
       "32                 0.0744     0.0010   -0.0096  0.0117 -0.0996  0.2242   \n",
       "33                 0.0128    -0.0213   -0.0266  0.0391 -0.0310  0.0553   \n",
       "34                 0.0437    -0.0540    0.0211  0.0486 -0.0679  0.1180   \n",
       "35                 0.0732    -0.0261    0.0581  0.0853 -0.0799  0.1355   \n",
       "36                -0.0168    -0.0765   -0.0045  0.0160 -0.0689 -0.0682   \n",
       "...                   ...        ...       ...     ...     ...     ...   \n",
       "wiley             -0.0183     0.0073   -0.0103  0.0347  0.0425 -0.0132   \n",
       "wiley periodicals -0.0305    -0.0018   -0.0244  0.0208  0.0406 -0.0210   \n",
       "wise              -0.0340    -0.1577    0.0342  0.0960 -0.1139  0.0167   \n",
       "wm                 0.0331     0.1518   -0.2238 -0.2011  0.0332  0.0035   \n",
       "wm task            0.0397     0.1736   -0.1292 -0.1199  0.0352 -0.0291   \n",
       "women              0.0001    -0.1230    0.2077  0.2691 -0.1094  0.2718   \n",
       "word               0.0046     0.0619    0.0998 -0.1460  0.1639 -0.1640   \n",
       "word form         -0.0654     0.0046    0.0100 -0.0874  0.1809 -0.2237   \n",
       "word pairs         0.1854     0.1502    0.1099  0.0020  0.0934  0.0583   \n",
       "word recognition  -0.0264    -0.0033    0.0427 -0.0686  0.0855 -0.1074   \n",
       "words              0.0103     0.0203    0.1878 -0.0413  0.1411 -0.1465   \n",
       "work               0.0386    -0.0612    0.0145 -0.0398 -0.0817  0.0608   \n",
       "working            0.0420     0.2498   -0.2408 -0.3057  0.0864 -0.0645   \n",
       "working memory     0.0417     0.2520   -0.2493 -0.3093  0.0866 -0.0645   \n",
       "world              0.1024     0.1007    0.1176  0.0343  0.0967  0.0363   \n",
       "worse              0.0882     0.0411    0.0752  0.0521 -0.0435  0.1366   \n",
       "written           -0.0446    -0.0119    0.0959 -0.0457  0.1050 -0.1532   \n",
       "year               0.0058     0.0063    0.1416  0.1332  0.0241  0.0679   \n",
       "year old           0.0247     0.0489    0.0606  0.0164  0.0647  0.0265   \n",
       "years             -0.0401    -0.1131    0.0357  0.0437 -0.0881  0.0111   \n",
       "yield             -0.0314    -0.0882   -0.0224  0.0225 -0.1015 -0.0320   \n",
       "yielded           -0.0506     0.0414   -0.0509 -0.0649  0.1129 -0.1612   \n",
       "young              0.0841     0.0083    0.0959  0.1452  0.0239  0.0915   \n",
       "young adults       0.0517    -0.0163    0.0655  0.1306 -0.0160  0.0843   \n",
       "young healthy      0.0009    -0.0259   -0.0278 -0.0059 -0.0570  0.0153   \n",
       "younger            0.1379     0.1842    0.0411 -0.0078  0.1278  0.0770   \n",
       "younger adults     0.1400     0.1529    0.0359  0.0015  0.0614  0.0946   \n",
       "zone              -0.0861    -0.1109   -0.0903 -0.1201 -0.1155 -0.0737   \n",
       "FALSE              0.2069     0.1074    0.2323  0.0962  0.0132  0.1784   \n",
       "TRUE               0.1548     0.1027    0.0723  0.0018  0.0872  0.0826   \n",
       "\n",
       "                   thalamus      mt  hypothalamus  angular   dlpfc  cerebellum  \n",
       "feature                                                                         \n",
       "1                    0.0781 -0.2110        0.1178  -0.1458 -0.1112      0.1668  \n",
       "1                    0.0127 -0.1755        0.0054  -0.0697 -0.0815      0.0313  \n",
       "5                    0.0215 -0.2886        0.0680  -0.2224 -0.2445      0.1712  \n",
       "10                   0.0533 -0.2550        0.1129  -0.0298  0.0549     -0.0935  \n",
       "11                  -0.0362 -0.1731        0.0579  -0.1718 -0.1331      0.0606  \n",
       "12                   0.1265 -0.1344        0.0048  -0.1212  0.0640      0.0319  \n",
       "13                   0.0484 -0.1787        0.0946  -0.1193 -0.0813      0.1048  \n",
       "14                   0.0165 -0.0750        0.0523  -0.0831 -0.0918      0.0806  \n",
       "15                   0.0303 -0.1622        0.0725  -0.1157 -0.0907     -0.0058  \n",
       "16                   0.0295 -0.1833        0.1471  -0.0669 -0.0827      0.0918  \n",
       "17                  -0.0893 -0.1256        0.0034  -0.0494 -0.1205      0.0703  \n",
       "18                  -0.0851 -0.2238        0.0758  -0.0372 -0.0942     -0.0594  \n",
       "19                  -0.1059 -0.0227        0.0600  -0.0260 -0.1228     -0.0627  \n",
       "20                  -0.0290 -0.1600        0.1120  -0.1547 -0.1812      0.1410  \n",
       "21                  -0.0226 -0.2061        0.1615  -0.0943 -0.1233      0.0134  \n",
       "22                  -0.0421 -0.1913        0.0901  -0.0821 -0.1468     -0.0357  \n",
       "23                   0.0147 -0.1383        0.0682  -0.1337 -0.1529      0.1126  \n",
       "24                   0.0186 -0.1538        0.0857  -0.0888 -0.0517     -0.0139  \n",
       "25                  -0.0091 -0.2191        0.1340  -0.0666 -0.1492     -0.0423  \n",
       "26                   0.0181 -0.1663        0.0847  -0.0950 -0.1319      0.0861  \n",
       "27                   0.0133 -0.2116        0.0547  -0.1053 -0.1477      0.1350  \n",
       "28                  -0.0325 -0.1540        0.1016  -0.0524 -0.1074     -0.0175  \n",
       "29                  -0.0308 -0.1433        0.0821  -0.0018 -0.0803     -0.0175  \n",
       "30                   0.0002 -0.1565        0.0110  -0.1711 -0.1773      0.1151  \n",
       "31                   0.0174 -0.1540        0.0418  -0.0571 -0.0515      0.0278  \n",
       "32                   0.0820 -0.1779        0.1184  -0.0258  0.0338     -0.0826  \n",
       "33                  -0.0023 -0.1142        0.0492  -0.0615 -0.0570     -0.0286  \n",
       "34                  -0.0063 -0.1577        0.0514  -0.0462 -0.0741     -0.0055  \n",
       "35                   0.0134 -0.1152        0.1087  -0.0194 -0.0383      0.0034  \n",
       "36                   0.0176 -0.0823        0.0192  -0.0398 -0.0686      0.0267  \n",
       "...                     ...     ...           ...      ...     ...         ...  \n",
       "wiley               -0.1049 -0.0157       -0.0363  -0.0064 -0.1048      0.0425  \n",
       "wiley periodicals   -0.0912 -0.0232       -0.0364  -0.0148 -0.1070      0.0583  \n",
       "wise                -0.0950 -0.2049        0.0780  -0.1634 -0.2621     -0.0338  \n",
       "wm                   0.0684  0.0568       -0.1046   0.0810  0.3086     -0.0516  \n",
       "wm task              0.0858  0.0880       -0.0917   0.1094  0.2414     -0.0190  \n",
       "women                0.1119 -0.1286        0.2970  -0.1358 -0.0708      0.0035  \n",
       "word                -0.0926  0.3521       -0.2327   0.2701  0.2475     -0.1083  \n",
       "word form           -0.1028  0.4285       -0.1535   0.0996  0.0441      0.0092  \n",
       "word pairs          -0.0317  0.0618       -0.0599   0.2031  0.1428     -0.0883  \n",
       "word recognition    -0.1221  0.2210       -0.1292   0.0986  0.0545     -0.0647  \n",
       "words               -0.1345  0.2815       -0.1691   0.2329  0.1352     -0.0973  \n",
       "work                 0.0051 -0.1506        0.0189   0.0022 -0.0588     -0.0735  \n",
       "working              0.1108  0.1467       -0.2384   0.2415  0.5258     -0.0222  \n",
       "working memory       0.1141  0.1480       -0.2422   0.2379  0.5295     -0.0227  \n",
       "world               -0.1174  0.0096       -0.0217   0.1148 -0.0861     -0.0385  \n",
       "worse               -0.0837 -0.1033        0.0341   0.0856 -0.0072     -0.0608  \n",
       "written             -0.0798  0.2781       -0.0941   0.0818  0.0415      0.0004  \n",
       "year                -0.1237 -0.0044        0.0402  -0.0061 -0.1153     -0.0578  \n",
       "year old            -0.0736  0.0501       -0.0071   0.0044 -0.0375     -0.0728  \n",
       "years               -0.0462 -0.2076        0.0692  -0.1276 -0.2056      0.0679  \n",
       "yield               -0.0249 -0.0516        0.0501  -0.0622 -0.0840      0.0171  \n",
       "yielded             -0.0081  0.2293       -0.0751   0.0178  0.0081      0.0279  \n",
       "young               -0.0408 -0.1450        0.0701   0.0366 -0.0853      0.1516  \n",
       "young adults        -0.0375 -0.1365        0.0496   0.0191 -0.0776      0.0116  \n",
       "young healthy        0.0786 -0.1285        0.0020  -0.0318 -0.0115      0.2458  \n",
       "younger             -0.0028  0.0471       -0.0568   0.1872  0.1146     -0.0695  \n",
       "younger adults      -0.0442 -0.0012       -0.0255   0.1554  0.0550     -0.0708  \n",
       "zone                 0.0522 -0.1108       -0.0644  -0.0614 -0.0091      0.0915  \n",
       "FALSE               -0.0107  0.0016        0.0212   0.2392  0.1349     -0.1346  \n",
       "TRUE                -0.0164  0.0131        0.0002   0.1778  0.0852     -0.0801  \n",
       "\n",
       "[3169 rows x 16 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurosynth_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_long = neurosynth_decoding.melt(id_vars='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_long.to_csv('{0}/NS/all_long.csv'.format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc_df = pd.read_csv('/Users/Katie/Dropbox/Data/NSvBM-decoding/pc-ns-terms.csv', \n",
    "                 sep=',', index_col=0)\n",
    "\n",
    "bd_df = pd.read_csv('/Users/Katie/Dropbox/Data/NSvBM-decoding/bd-ns-terms.csv', \n",
    "                 sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Significant BM PCs and BDs for each ROI\n",
    "#Separated by fwd and rev inference\n",
    "fusiform_fwd_bd = ['Action.Motor Learning', 'Cognition.Language', 'Emotion.Fear',\n",
    "                   'Perception.Vision.Shape', 'Emotion.Anger', 'Perception.Vision',\n",
    "                   'Cognition.Language.Semantics']\n",
    "fusiform_fwd_pc = ['Drawing', 'Naming (Covert)', 'Naming (Overt)', \n",
    "                   'Face Monitor/Discrimination', 'Passive Viewing']\n",
    "fusiform_rev_bd = ['Cognition.Language.Semantics', 'Perception.Vision.Shape',\n",
    "                   'Perception.Vision', 'Emotion.Fear', 'Cognition.Language',\n",
    "                   'Action.Motor Learning']\n",
    "fusiform_rev_pc = ['Face Monitor/Discrimination', 'Passive Viewing', 'Naming (Covert)',\n",
    "                   'Naming (Overt)', 'Reward', 'Drawing']\n",
    "\n",
    "dlpfc_fwd_bd = ['Cognition.Memory.Working', 'Action.Inhibition']\n",
    "dlpfc_fwd_pc = ['Stroop-Color', 'n-Back', 'Delayed Match to Sample']\n",
    "dlpfc_rev_bd = ['Cognition.Memory.Working', 'Action.Inhibition']\n",
    "dlpfc_rev_pc = ['n-Back', 'Face Monitor/Discrimination', 'Delayed Match to Sample', \n",
    "                'Counting/Calculation', 'Go/No-Go', 'Passive Viewing', \n",
    "                'Stroop-Color', 'Passive Listening', 'Flexion/Extension',\n",
    "                'Naming (Overt)']\n",
    "\n",
    "cerebellum_fwd_bd = ['Interoception', 'Interoception.Gastrointestinal/ Genitourinary (GI/GU)', 'Perception.Somesthesis.Pain']\n",
    "cerebellum_fwd_pc = ['Hypercapnia/Air Hunger', 'Visual Pursuit/Tracking', 'Pain Monitor/Discrimination']\n",
    "cerebellum_rev_bd = ['Perception.Somesthesis.Pain', 'Interoception.Gastrointestinal/ Genitourinary (GI/GU)', 'Interoception']\n",
    "cerebellum_rev_pc = ['Pain Monitor/Discrimination', 'Visual Pursuit/Tracking', \n",
    "                     'Micturition', 'Acupuncture', 'Hypercapnia/Air Hunger', 'Isometric Force']\n",
    "\n",
    "angular_fwd_bd = ['Cognition.Social Cognition', 'Cognition.Reasoning', \n",
    "                  'Cognition.Memory.Explicit', 'Cognition']\n",
    "angular_fwd_pc = ['Deception', 'Theory of Mind', 'Semantic Monitor/Discrimination']\n",
    "angular_rev_bd = ['Cognition', 'Cognition.Memory.Explicit', 'Cognition.Social Cognition',\n",
    "                  'Cognition.Reasoning']\n",
    "angular_rev_pc = ['Semantic Monitor/Discrimination', 'Theory of Mind', 'Deception',\n",
    "                  'Delay Discounting']\n",
    "\n",
    "thalamus_fwd_bd = ['Perception.Somesthesis.Pain', 'Cognition', 'Action.Execution', \n",
    "                   'Emotion']\n",
    "thalamus_fwd_pc = ['Reward', 'Pain Monitor/Discrimination']\n",
    "thalamus_rev_bd = ['Cognition', 'Action.Execution', 'Perception.Somesthesis.Pain', \n",
    "                   'Action.Execution.Speech']\n",
    "thalamus_rev_pc = ['Reward', 'Pain Monitor/Discrimination', 'Semantic Monitor/Discrimination',\n",
    "                   'Face Monitor/Discrimination', 'Go/No-Go', 'Emotion Induction', \n",
    "                   'Theory of Mind']\n",
    "\n",
    "hypothal_fwd_bd = ['Perception.Olfaction', 'Interoception.Sexuality', \n",
    "                   'Emotion']\n",
    "hypothal_fwd_pc = ['Olfactory Monitor/Discrimination', 'Pitch Monitor/Discrimination', \n",
    "                   'Film Viewing', 'Emotion Induction']\n",
    "hypothal_rev_bd = ['Emotion', 'Perception.Olfaction', 'Interoception.Sexuality']\n",
    "hypothal_rev_pc = ['Olfactory Monitor/Discrimination', 'Film Viewing', 'Pitch Monitor/Discrimination']\n",
    "\n",
    "acc_fwd_bd = ['Interoception.Thirst', 'Perception.Gustation', 'Emotion.Fear',\n",
    "              'Interoception.Gastrointestinal/ Genitourinary (GI/GU)', 'Perception.Olfaction', 'Interoception.Sexuality',\n",
    "              'Emotion.Sadness', 'Cognition.Somatic', 'Cognition', 'Emotion.Happiness',\n",
    "              'Emotion', 'Cognition.Social Cognition', 'Perception.Somesthesis.Pain']\n",
    "acc_fwd_pc = ['Taste', 'Reward', 'Face Monitor/Discrimination']\n",
    "acc_rev_bd = ['Emotion', 'Cognition', 'Cognition.Social Cognition', 'Perception.Gustation',\n",
    "              'Perception.Somesthesis.Pain', 'Emotion.Fear', 'Action.Inhibition',\n",
    "              'Interoception.Sexuality', 'Interoception.Thirst']\n",
    "acc_rev_pc = ['Reward', 'Pain Monitor/Discrimination', 'Finger Tapping/Button Press',\n",
    "              'Taste', 'Delay Discounting']\n",
    "\n",
    "mt_fwd_bd = ['Cognition.Memory', 'Action.Observation', 'Perception.Vision.Shape',\n",
    "             'Emotion.Anger', 'Emotion.Disgust', 'Interoception.Sexuality', 'Cognition.Spatial',\n",
    "             'Perception.Vision', 'Cognition.Language.Orthography', 'Perception.Vision',\n",
    "             'Perception.Vision.Motion']\n",
    "mt_fwd_pc = ['Naming (Covert)', 'Affective Pictures', 'Visual Pursuit/Tracking',\n",
    "             'Mental Rotation', 'Film Viewing', 'Visual Object Identification',\n",
    "             'Encoding', 'Visuospatial Attention', 'Face Monitor/Discrimination']\n",
    "mt_rev_bd = ['Cognition.Attention', 'Perception.Vision.Shape', 'Perception.Vision',\n",
    "             'Cognition.Language.Orthography', 'Perception.Vision.Motion', 'Cognition.Spatial',\n",
    "             'Action.Observation', 'Interoception.Sexuality', 'Emotion.Disgust', \n",
    "             'Cognition.Memory']\n",
    "mt_rev_pc = ['Passive Viewing', 'Visuospatial Attention', 'Film Viewing', 'Encoding',\n",
    "             'Reward', 'Delayed Match to Sample', 'Mental Rotation', 'Visual Object Identification',\n",
    "             'Affective Pictures', 'Reading (Covert)', 'Naming (Covert)', 'Visual Pursuit/Tracking',\n",
    "             'Pain Monitor/Discrimination']\n",
    "\n",
    "ofc_fwd_bd = ['Perception.Somesthesis.Pain', 'Emotion']\n",
    "ofc_fwd_pc = ['Pain Monitor/Discrimination']\n",
    "ofc_rev_bd = ['Emotion', 'Perception.Somesthesis.Pain', 'Cognition.Music']\n",
    "ofc_rev_pc = ['Pain Monitor/Discrimination', 'Music Comprehension', 'Deception', 'Stroop-Other']\n",
    "\n",
    "pcc_fwd_bd = ['Cognition', 'Cognition.Memory.Explicit', 'Emotion']\n",
    "pcc_fwd_pc = ['Delay Discounting', 'Reward']\n",
    "pcc_rev_bd = ['Emotion', 'Cognition', 'Cognition.Memory.Explicit', 'Perception.Somesthesis.Pain']\n",
    "pcc_rev_pc = ['Reward', 'Cued Explicit Recognition/Recall', 'Delay Discounting', 'Deception']\n",
    "\n",
    "paracentral_fwd_bd = ['Action.Imagination', 'Action.Execution']\n",
    "paracentral_fwd_pc = ['Imagined Movement', 'Oddball Discrimination', 'Reward']\n",
    "paracentral_rev_bd = ['Action.Imagination', 'Action.Execution']\n",
    "paracentral_rev_pc = ['Reward', 'Imagined Movement', 'Sequence Recall/Learning', 'Oddball Discrimination']\n",
    "\n",
    "precuneus_fwd_bd = ['Cognition.Social Cognition', 'Cognition.Reasoning',\n",
    "                    'Cognition.Memory.Explicit', 'Perception.Vision']\n",
    "precuneus_fwd_pc = ['Self-Reflection', 'Theory of Mind', 'Cued Explicit Recognition/Recall',\n",
    "                    'Visuospatial Attention']\n",
    "precuneus_rev_bd = ['Cognition.Attention', 'Cognition.Memory.Explicit', 'Cognition.Social Cognition',\n",
    "                    'Cognition.Memory.Working', 'Cognition.Reasoning', 'Action.Inhibition', \n",
    "                    'Perception.Vision.Motion']\n",
    "precuneus_rev_pc = ['Theory of Mind','Visuospatial Attention', 'Cued Explicit Recognition/Recall',\n",
    "                    'Saccades', 'Self-Reflection', 'Deception', 'Hand-Eye Coordination']\n",
    "\n",
    "temporal_fwd_bd = ['Emotion.Sadness', 'Cognition.Social Cognition', 'Cognition.Language',\n",
    "                   'Emotion.Happiness', 'Emotion.Anger', 'Cognition.Language.Syntax',\n",
    "                   'Cognition.Memory', 'Emotion.Disgust', 'Cognition.Reasoning',\n",
    "                   'Cognition.Language.Semantics', 'Cognition.Memory.Explicit', 'Emotion']\n",
    "temporal_fwd_pc = ['Theory of Mind', 'Figurative Language', 'Recitation/Repetition (Covert)',\n",
    "                   'Autobiographical Recall', 'Emotion Induction', 'Film Viewing',\n",
    "                   'Face Monitor/Discrimination', 'Semantic Monitor/Discrimination']\n",
    "temporal_rev_bd = ['Emotion', 'Cognition.Language.Semantics', 'Cognition.Social Cognition',\n",
    "                   'Cognition.Memory.Explicit', 'Emotion.Sadness', 'Emotion.Happiness',\n",
    "                   'Cognition.Language', 'Cognition.Memory']\n",
    "temporal_rev_pc = ['Semantic Monitor/Discrimination', 'Face Monitor/Discrimination',\n",
    "                   'Theory of Mind', 'Film Viewing', 'Autobiographical Recall',\n",
    "                   'Figurative Language', 'Recitation/Repetition (Covert)']\n",
    "\n",
    "uncus_fwd_bd = ['Perception.Olfaction', 'Cognition.Memory', 'Emotion.Anger', 'Interoception',\n",
    "                'Emotion.Fear', 'Interoception.Sexuality', 'Emotion.Sadness',\n",
    "                'Emotion.Happiness', 'Emotion.Disgust', 'Perception.Gustation', 'Emotion',\n",
    "                'Cognition.Memory.Explicit', 'Cognition']\n",
    "uncus_fwd_pc = ['Olfactory Monitor/Discrimination', 'Classical Conditioning', 'Hypercapnia/Air Hunger',\n",
    "                'Affective Pictures', 'Emotion Induction', 'Face Monitor/Discrimination', 'Encoding',\n",
    "                'Passive Viewing']\n",
    "uncus_rev_bd = ['Emotion', 'Cognition.Memory.Explicit', 'Emotion.Fear', 'Perception.Olfaction',\n",
    "                'Cognition.Memory', 'Interoception.Sexuality', 'Emotion.Anger', 'Emotion.Disgust',\n",
    "                'Interoception']\n",
    "uncus_rev_pc = ['Face Monitor/Discrimination', 'Emotion Induction', 'Passive Viewing',\n",
    "                'Encoding', 'Classical Conditioning', 'Affective Pictures', \n",
    "                'Olfactory Monitor/Discrimination', 'Film Viewing', 'Hypercapnia/Air Hunger']\n",
    "\n",
    "visual_fwd_bd = ['Interoception.Sleep', 'Cognition.Somatic', 'Emotion.Anxiety', \n",
    "                 'Emotion.Disgust', 'Perception.Vision.Motion', 'Cognition.Language.Semantics']\n",
    "visual_fwd_pc = ['Flashing Checkerboard', 'Naming (Overt)', 'Saccades']\n",
    "visual_rev_bd = ['Perception.Vision.Motion', 'Emotion.Disgust', 'Cognition.Somatic']\n",
    "visual_rev_pc = ['Finger Tapping/Button Press', 'Naming (Overt)', 'Saccades',\n",
    "                 'Visual Pursuit/Tracking', 'Drawing', 'Passive Listening',\n",
    "                 'Flashing Checkerboard', 'Driving']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roi_list_bd = [dlpfc_fwd_bd, dlpfc_rev_bd, cerebellum_fwd_bd, cerebellum_rev_bd,\n",
    "               angular_fwd_bd, angular_rev_bd, thalamus_fwd_bd, thalamus_rev_bd, \n",
    "               hypothal_fwd_bd, hypothal_rev_bd, acc_fwd_bd, acc_rev_bd,\n",
    "               mt_fwd_bd, mt_rev_bd, fusiform_fwd_bd, fusiform_rev_bd, ofc_fwd_bd,\n",
    "               ofc_rev_bd, pcc_fwd_bd, pcc_rev_bd, paracentral_fwd_bd, paracentral_rev_bd,\n",
    "               precuneus_fwd_bd, precuneus_rev_bd, temporal_fwd_bd, temporal_rev_bd,\n",
    "               uncus_fwd_bd, uncus_rev_bd, visual_fwd_bd, visual_rev_bd]\n",
    "roi_list_pc = [dlpfc_fwd_pc, dlpfc_rev_pc, cerebellum_fwd_pc, cerebellum_rev_pc, \n",
    "               angular_fwd_pc, angular_rev_pc, thalamus_fwd_pc, thalamus_rev_pc, \n",
    "               hypothal_fwd_pc, hypothal_rev_pc, acc_fwd_pc, acc_rev_pc, \n",
    "               mt_fwd_pc, mt_rev_pc, fusiform_fwd_pc, fusiform_rev_pc, ofc_fwd_pc,\n",
    "               ofc_rev_pc, pcc_fwd_pc, pcc_rev_pc, paracentral_fwd_pc, paracentral_rev_pc,\n",
    "               precuneus_fwd_pc, precuneus_rev_pc, temporal_fwd_pc, temporal_rev_pc,\n",
    "               uncus_fwd_pc, uncus_rev_pc, visual_fwd_pc, visual_rev_pc]\n",
    "\n",
    "rois_bd = ['dlpfc_fwd_bd', 'dlpfc_rev_bd', 'cerebellum_fwd_bd', 'cerebellum_rev_bd',\n",
    "           'angular_fwd_bd', 'angular_rev_bd', 'thalamus_fwd_bd', 'thalamus_rev_bd',\n",
    "           'hypothal_fwd_bd', 'hypothal_rev_bd', 'acc_fwd_bd', 'acc_rev_bd',\n",
    "           'mt_fwd_bd', 'mt_rev_bd', 'fusiform_fwd_bd', 'fusiform_rev_bd', 'ofc_fwd_bd',\n",
    "           'ofc_rev_bd', 'pcc_fwd_bd', 'pcc_rev_bd', 'paracentral_fwd_bd', 'paracentral_rev_bd',\n",
    "           'precuneus_fwd_bd', 'precuneus_rev_bd', 'temporal_fwd_bd', 'temporal_rev_bd',\n",
    "           'uncus_fwd_bd', 'uncus_rev_bd', 'visual_fwd_bd', 'visual_rev_bd']\n",
    "rois_pc = ['dlpfc_fwd_pc', 'dlpfc_rev_pc', 'cerebellum_fwd_pc', 'cerebellum_rev_pc',\n",
    "           'angular_fwd_pc', 'angular_rev_pc', 'thalamus_fwd_pc', 'thalamus_rev_pc',\n",
    "           'hypothal_fwd_pc', 'hypothal_rev_pc', 'acc_fwd_pc', 'acc_rev_pc',\n",
    "           'mt_fwd_pc', 'mt_rev_pc', 'fusiform_fwd_pc', 'fusiform_rev_pc', 'ofc_fwd_pc',\n",
    "           'ofc_rev_pc', 'pcc_fwd_pc', 'pcc_rev_pc', 'paracentral_fwd_pc', 'paracentral_rev_pc',\n",
    "           'precuneus_fwd_pc', 'precuneus_rev_pc', 'temporal_fwd_pc', 'temporal_rev_pc',\n",
    "           'uncus_fwd_pc', 'uncus_rev_pc', 'visual_fwd_pc', 'visual_rev_pc']\n",
    "\n",
    "rois = ['dlpfc', 'dlpfc', 'cerebellum', 'cerebellum', 'angular', 'angular', \n",
    "        'thalamus', 'thalamus', 'hypothalamus', 'hypothalamus', 'acc', 'acc',\n",
    "        'mt', 'mt', 'fusiform', 'fusiform', 'orbitofrontal', 'orbitofrontal', 'pcc', 'pcc',\n",
    "        'paracentral', 'paracentral', 'precuneus', 'precuneus', 'temporal', 'temporal',\n",
    "        'uncus', 'uncus', 'visual', 'visual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bd_num = {}\n",
    "pc_num = {}\n",
    "for i in np.arange(0, len(roi_list_bd)):\n",
    "    bd_num[rois_bd[i]] = len(roi_list_bd[i])\n",
    "    pc_num[rois_pc[i]] = len(roi_list_pc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_fwd_pc': 3,\n",
       " 'acc_rev_pc': 5,\n",
       " 'angular_fwd_pc': 3,\n",
       " 'angular_rev_pc': 4,\n",
       " 'cerebellum_fwd_pc': 3,\n",
       " 'cerebellum_rev_pc': 6,\n",
       " 'dlpfc_fwd_pc': 3,\n",
       " 'dlpfc_rev_pc': 10,\n",
       " 'fusiform_fwd_pc': 5,\n",
       " 'fusiform_rev_pc': 6,\n",
       " 'hypothal_fwd_pc': 4,\n",
       " 'hypothal_rev_pc': 3,\n",
       " 'mt_fwd_pc': 9,\n",
       " 'mt_rev_pc': 13,\n",
       " 'ofc_fwd_pc': 1,\n",
       " 'ofc_rev_pc': 4,\n",
       " 'paracentral_fwd_pc': 3,\n",
       " 'paracentral_rev_pc': 4,\n",
       " 'pcc_fwd_pc': 2,\n",
       " 'pcc_rev_pc': 4,\n",
       " 'precuneus_fwd_pc': 4,\n",
       " 'precuneus_rev_pc': 7,\n",
       " 'temporal_fwd_pc': 8,\n",
       " 'temporal_rev_pc': 7,\n",
       " 'thalamus_fwd_pc': 2,\n",
       " 'thalamus_rev_pc': 7,\n",
       " 'uncus_fwd_pc': 8,\n",
       " 'uncus_rev_pc': 9,\n",
       " 'visual_fwd_pc': 3,\n",
       " 'visual_rev_pc': 8}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull the r values for the NS terms corresponding to the bds\n",
    "#return r values\n",
    "pos_ns_sig_bm = {}\n",
    "neg_ns_sig_bm = {}\n",
    "mean_r = {}\n",
    "for j in np.arange(0, len(roi_list_bd)):\n",
    "    r_vals = {}\n",
    "    terms = []\n",
    "    no_ns = []\n",
    "    pos_ns = ['term', 'r']\n",
    "    neg_ns = ['term', 'r']\n",
    "    r = []\n",
    "\n",
    "    for i in np.arange(0, len(roi_list_bd[j])):\n",
    "        if type(bd_df.loc[roi_list_bd[j][i], 'Neurosynth Terms']) is str:\n",
    "            #grab ns terms associated with each bd\n",
    "            terms = bd_df.loc[roi_list_bd[j][i], 'Neurosynth Terms']\n",
    "            #split them from string into list\n",
    "            terms = terms.split(\", \")\n",
    "            for k in np.arange(0, len(terms)):\n",
    "                r_vals[roi_list_bd[j][i]] = terms\n",
    "                r_vals[terms[k]] = neurosynth_decoding.loc[terms[k], rois[j]]\n",
    "                r.append(neurosynth_decoding.loc[terms[k], rois[j]])\n",
    "                if neurosynth_decoding.loc[terms[k], rois[j]] > 0:\n",
    "                    pos_ns = np.vstack((pos_ns, [terms[k], neurosynth_decoding.loc[terms[k], rois[j]]]))\n",
    "                else:\n",
    "                    neg_ns = np.vstack((neg_ns, [terms[k], neurosynth_decoding.loc[terms[k], rois[j]]]))\n",
    "        else:\n",
    "            r_vals[roi_list_bd[j][i]] = 0\n",
    "            no_ns.append(roi_list_bd[j][i])\n",
    "        nopes[rois_bd[j]] = no_ns \n",
    "    mean_r[rois_bd[j]] = np.average(r)\n",
    "    pos_ns_sig_bm[rois_bd[j]] = pos_ns\n",
    "    neg_ns_sig_bm[rois_bd[j]] = neg_ns\n",
    "\n",
    "    df = pd.Series(r_vals)\n",
    "    df.to_csv('{0}/ns_rvals_{1}.csv'.format(data_dir, rois_bd[j]))\n",
    "\n",
    "pos_ns_df = pd.Series(pos_ns_sig_bm)\n",
    "pos_ns_df.to_csv('{0}/pos-ns-bd.csv'.format(data_dir))\n",
    "neg_ns_df = pd.Series(neg_ns_sig_bm)\n",
    "neg_ns_df.to_csv('{0}/neg-ns-bd.csv'.format(data_dir))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull the r values for the NS terms corresponding to the pcs\n",
    "#return r values\n",
    "pos_ns_sig_bm = {}\n",
    "neg_ns_sig_bm = {}\n",
    "for j in np.arange(0, len(roi_list_pc)):\n",
    "    r_vals = {}\n",
    "    terms = []\n",
    "    no_ns = []\n",
    "    pos_ns = ['term', 'r']\n",
    "    neg_ns = ['term', 'r']\n",
    "    r = []\n",
    "\n",
    "    for i in np.arange(0, len(roi_list_pc[j])):\n",
    "        if type(pc_df.loc[roi_list_pc[j][i], 'Neurosynth Terms']) is str:\n",
    "            #grab ns terms associated with each pc\n",
    "            terms = pc_df.loc[roi_list_pc[j][i], 'Neurosynth Terms']\n",
    "            #split them from string into list\n",
    "            terms = terms.split(\", \")\n",
    "            for k in np.arange(0, len(terms)):\n",
    "                r_vals[roi_list_pc[j][i]] = terms\n",
    "                r_vals[terms[k]] = neurosynth_decoding.loc[terms[k], rois[j]]\n",
    "                #r.append(neurosynth_decoding.loc[terms[k], rois[j]])\n",
    "                if neurosynth_decoding.loc[terms[k], rois[j]] > 0:\n",
    "                    pos_ns = np.vstack((pos_ns, [terms[k], neurosynth_decoding.loc[terms[k], rois[j]]]))\n",
    "                else:\n",
    "                    neg_ns = np.vstack((neg_ns, [terms[k], neurosynth_decoding.loc[terms[k], rois[j]]]))\n",
    "        else:\n",
    "            r_vals[roi_list_pc[j][i]] = 0\n",
    "            no_ns.append(roi_list_pc[j][i])\n",
    "        nopes[rois_pc[j]] = no_ns \n",
    "    #mean_r[rois_pc[j]] = np.average(r)\n",
    "    pos_ns_sig_bm[rois_pc[j]] = pos_ns\n",
    "    neg_ns_sig_bm[rois_pc[j]] = neg_ns\n",
    "\n",
    "    df = pd.Series(r_vals)\n",
    "    df.to_csv('{0}/ns_rvals_{1}.csv'.format(data_dir, rois_pc[j]))\n",
    "\n",
    "pos_ns_df = pd.Series(pos_ns_sig_bm)\n",
    "pos_ns_df.to_csv('{0}/pos-ns-pc.csv'.format(data_dir))\n",
    "neg_ns_df = pd.Series(neg_ns_sig_bm)\n",
    "neg_ns_df.to_csv('{0}/neg-ns-pc.csv'.format(data_dir))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 4. Return the significant BM terms that are <br>\n",
    "###    a. Represented by NS terms with (+) correlations<br>\n",
    "###    b. Represented by NS terms with (-) correlations<br>\n",
    "###    c. Not represented by NS terms<br>\n",
    "#4A AND 4B ARE RETURNED BY THE PREVIOUS TWO CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_fwd_bd': 0.15713333333333332,\n",
       " 'acc_rev_bd': 0.17314615384615387,\n",
       " 'angular_fwd_bd': 0.16628000000000001,\n",
       " 'angular_rev_bd': 0.16628000000000001,\n",
       " 'cerebellum_fwd_bd': 0.10233333333333333,\n",
       " 'cerebellum_rev_bd': 0.10233333333333333,\n",
       " 'dlpfc_fwd_bd': 0.32246666666666668,\n",
       " 'dlpfc_rev_bd': 0.32246666666666668,\n",
       " 'fusiform_fwd_bd': 0.17371666666666666,\n",
       " 'fusiform_rev_bd': 0.187,\n",
       " 'hypothal_fwd_bd': 0.28014444444444447,\n",
       " 'hypothal_rev_bd': 0.28014444444444442,\n",
       " 'mt_fwd_bd': 0.2418595238095238,\n",
       " 'mt_rev_bd': 0.22471315789473678,\n",
       " 'ofc_fwd_bd': 0.1101,\n",
       " 'ofc_rev_bd': 0.094118181818181823,\n",
       " 'paracentral_fwd_bd': 0.30015000000000003,\n",
       " 'paracentral_rev_bd': 0.30015000000000003,\n",
       " 'pcc_fwd_bd': 0.099286666666666676,\n",
       " 'pcc_rev_bd': 0.09322941176470588,\n",
       " 'precuneus_fwd_bd': 0.10653333333333335,\n",
       " 'precuneus_rev_bd': 0.14133500000000002,\n",
       " 'temporal_fwd_bd': 0.22773428571428569,\n",
       " 'temporal_rev_bd': 0.25346153846153852,\n",
       " 'thalamus_fwd_bd': 0.0038555555555555504,\n",
       " 'thalamus_rev_bd': 0.044628571428571427,\n",
       " 'uncus_fwd_bd': 0.23244705882352945,\n",
       " 'uncus_rev_bd': 0.30763043478260871,\n",
       " 'visual_fwd_bd': 0.017499999999999998,\n",
       " 'visual_rev_bd': 0.081333333333333327}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4c. not in NS\n",
    "nope_df = pd.Series(nopes, index = nopes.keys())\n",
    "nope_df.to_csv('{0}/sig_bm_not_in_ns.csv'.format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roi_list = ['dlpfc', 'cerebellum', 'angular', 'thalamus', 'hypothalamus', 'acc',\n",
    "            'mt', 'fusiform', 'orbitofrontal', 'pcc', 'paracentral', 'precuneus', \n",
    "            'temporal', 'uncus', 'visual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#significant BM terms that are represented by +/- NS terms (4a, 4b)\n",
    "#1. read in the NS terms assoc. with significant BM terms\n",
    "#2. pull out the ones with (+) r values\n",
    "#3. pull out the ones with (-) r values\n",
    "#4. match the +/- terms with their BM counterparts\n",
    "#5. write out list of \n",
    "for i in roi_list:\n",
    "    fwd_bd = pd.read_csv('{0}/{1}_fwd_bd.csv'.format(data_dir, i), header=None, index_col=0)\n",
    "    pos_bm = fwd_bd.index[fwd_bd[1] > 0].tolist()\n",
    "    neg_bm = fwd_bd.index[fwd_bd[1] < 0].tolist()\n",
    "    #find the matching BM BDs for each\n",
    "    absent = {}\n",
    "    for j in np.arange(0, len(pos_bm)):\n",
    "        no = []\n",
    "        #print pc_df['Neurosynth Terms'][pos_bm[j]]\n",
    "        try:\n",
    "            pc_df.index(pc_df['Neurosynth Terms'][pos_bm[j]])\n",
    "            print pc_df.index(pc_df['Neurosynth Terms'][pos_bm[j]])\n",
    "        except KeyError:\n",
    "            no.append(pos_bm[j])\n",
    "        print i\n",
    "        print no\n",
    "        #absent[rois[i]] = no\n",
    "    \n",
    "    rev_bd = pd.read_csv('{0}/{1}_rev_bd.csv'.format(data_dir, i), header=None, index_col=0)\n",
    "    fwd_pc = pd.read_csv('{0}/{1}_fwd_pc.csv'.format(data_dir, i), header=None, index_col=0)\n",
    "    rev_pc = pd.read_csv('{0}/{1}_rev_pc.csv'.format(data_dir, i), header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 5. Return the positive NS terms that are<br>\n",
    "###    a. Not significant in BM<br>\n",
    "###    b. Significant in BM (same as 4a)<br>\n",
    "###    c. Not in BM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NS terms with positive correlation values\n",
    "pos_r = {}\n",
    "for roi in roi_list:\n",
    "    pos_r[roi] = neurosynth_decoding.index[neurosynth_decoding[roi] > 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in \"not in BM\" and \"yes in BM\" NS terms\n",
    "import csv\n",
    "with open('/Users/Katie/Dropbox/Data/NSvBM-decoding/fxl-ns-in-bm.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    ns_in_bm = list(reader)\n",
    "ns_in_bm = sum(ns_in_bm, [])\n",
    "\n",
    "import csv\n",
    "with open('/Users/Katie/Dropbox/Data/NSvBM-decoding/fxl-ns-not-in-bm.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    ns_not_bm = list(reader)\n",
    "ns_not_bm = sum(ns_not_bm, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find positive NS terms that are not in BM (5c)\n",
    "five_c = {}\n",
    "\n",
    "for roi in roi_list:\n",
    "    five_c[roi] = list(set(pos_r[roi]) & set(ns_not_bm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dlpfc',\n",
       " 'cerebellum',\n",
       " 'angular',\n",
       " 'thalamus',\n",
       " 'hypothalamus',\n",
       " 'acc',\n",
       " 'mt',\n",
       " 'fusiform',\n",
       " 'orbitofrontal',\n",
       " 'pcc',\n",
       " 'paracentral',\n",
       " 'precuneus',\n",
       " 'temporal',\n",
       " 'uncus',\n",
       " 'visual']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "five_c_num = []\n",
    "m = 0\n",
    "for roi in roi_list:\n",
    "    five_c_num.append(len(five_c[roi]))\n",
    "    m += len(five_c[roi])\n",
    "m = m/len(roi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n",
      "[408, 308, 426, 392, 351, 410, 410, 402, 434, 441, 356, 441, 400, 364, 435]\n"
     ]
    }
   ],
   "source": [
    "print m\n",
    "print five_c_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ns_df = pd.read_csv('{0}/NS/all.csv'.format(data_dir), header=0, index_col=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ns_df = all_ns_df.drop('feature', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x11b5d7bd0>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAB7CAYAAAABvKOVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEIIIXWCAjRCCCGEEEIIqRP/P496HaWZSiW3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a64ad50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "g = sns.FacetGrid(all_ns_df, row=\"type\", hue=\"type\", aspect=15, size=.5, palette=pal)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"fusiform\", clip_on=False, shade=True, alpha=1, lw=1.5, bw=.2)\n",
    "g.map(sns.kdeplot, \"fusiform\", clip_on=False, color=\"w\", lw=2, bw=.2)\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color, \n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "g.map(label, \"fusiform\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.fig.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play will with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.set(xlim=[-0.7, 0.6])\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>roi</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.2374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.0313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Non-content</td>\n",
       "      <td>fusiform</td>\n",
       "      <td>-0.1252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type       roi   value\n",
       "0  Non-content  fusiform -0.1550\n",
       "1  Non-content  fusiform -0.1315\n",
       "2  Non-content  fusiform -0.1594\n",
       "3  Non-content  fusiform -0.2374\n",
       "4  Non-content  fusiform -0.1152\n",
       "5  Non-content  fusiform -0.1239\n",
       "6  Non-content  fusiform -0.1416\n",
       "7  Non-content  fusiform -0.0313\n",
       "8  Non-content  fusiform -0.1521\n",
       "9  Non-content  fusiform -0.1252"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Melt\" the dataset to \"long-form\" or \"tidy\" representation\n",
    "tidy_all_df = pd.melt(all_ns_df, \"type\", var_name=\"roi\")\n",
    "tidy_all_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(55,5))\n",
    "sns.swarmplot(x=\"roi\", y=\"value\", hue=\"type\", palette='husl', data=tidy_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fus_categ = all_ns_df.pivot(columns='type', values='fusiform')\n",
    "ofc_categ = all_ns_df.pivot(columns='type', values='orbitofrontal')\n",
    "pcg_categ = all_ns_df.pivot(columns='type', values='paracentral')\n",
    "pcc_categ = all_ns_df.pivot(columns='type', values='pcc')\n",
    "prc_categ = all_ns_df.pivot(columns='type', values='precuneus')\n",
    "tem_categ = all_ns_df.pivot(columns='type', values='temporal')\n",
    "unc_categ = all_ns_df.pivot(columns='type', values='uncus')\n",
    "vis_categ = all_ns_df.pivot(columns='type', values='visual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Katie/Dropbox/Data/NSvBM-decoding/decoded'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean FP</th>\n",
       "      <th>FP SD</th>\n",
       "      <th>Size (mm)</th>\n",
       "      <th>Anat?</th>\n",
       "      <th>Informative NS terms - pos</th>\n",
       "      <th>Informative NS terms top 50</th>\n",
       "      <th>Informative NS terms top 20</th>\n",
       "      <th># sig BM FWD BD</th>\n",
       "      <th>w/ no ns fwd bd</th>\n",
       "      <th>prop ns pos fwd bd</th>\n",
       "      <th>...</th>\n",
       "      <th># sig BM REV PC</th>\n",
       "      <th>w/ no ns rev pc</th>\n",
       "      <th>prop pos rev pc</th>\n",
       "      <th>mean r fwd bd</th>\n",
       "      <th>mean r rev bd</th>\n",
       "      <th>mean r fwd pc</th>\n",
       "      <th>mean r rev pc</th>\n",
       "      <th>mean of means</th>\n",
       "      <th>avg func ns top50</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>18.871341</td>\n",
       "      <td>7.903286</td>\n",
       "      <td>22048</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.157133</td>\n",
       "      <td>0.173140</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.162043</td>\n",
       "      <td>0.417895</td>\n",
       "      <td>0.231195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angular</th>\n",
       "      <td>30.083092</td>\n",
       "      <td>7.176601</td>\n",
       "      <td>2824</td>\n",
       "      <td>1</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166280</td>\n",
       "      <td>0.166280</td>\n",
       "      <td>0.246729</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>0.199044</td>\n",
       "      <td>0.354437</td>\n",
       "      <td>0.137550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cerebellum</th>\n",
       "      <td>15.613686</td>\n",
       "      <td>7.201233</td>\n",
       "      <td>17968</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102333</td>\n",
       "      <td>0.102333</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.126267</td>\n",
       "      <td>0.243906</td>\n",
       "      <td>0.093706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dlpfc</th>\n",
       "      <td>21.547704</td>\n",
       "      <td>7.180558</td>\n",
       "      <td>21584</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.263157895</td>\n",
       "      <td>0.322467</td>\n",
       "      <td>0.322467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.047241</td>\n",
       "      <td>0.149423</td>\n",
       "      <td>0.422167</td>\n",
       "      <td>0.469408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fusiform</th>\n",
       "      <td>14.882338</td>\n",
       "      <td>5.415142</td>\n",
       "      <td>4808</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.35</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.714285714</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.173671</td>\n",
       "      <td>0.211772</td>\n",
       "      <td>0.388636</td>\n",
       "      <td>0.214964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mean FP     FP SD  Size (mm)  Anat?  Informative NS terms - pos  \\\n",
       "Region                                                                          \n",
       "acc         18.871341  7.903286      22048      1                       0.400   \n",
       "angular     30.083092  7.176601       2824      1                       0.394   \n",
       "cerebellum  15.613686  7.201233      17968      0                       0.329   \n",
       "dlpfc       21.547704  7.180558      21584      0                       0.380   \n",
       "fusiform    14.882338  5.415142       4808      1                       0.387   \n",
       "\n",
       "            Informative NS terms top 50  Informative NS terms top 20  \\\n",
       "Region                                                                 \n",
       "acc                                0.40                         0.20   \n",
       "angular                            0.54                         0.50   \n",
       "cerebellum                         0.34                         0.15   \n",
       "dlpfc                              0.36                         0.40   \n",
       "fusiform                           0.56                         0.35   \n",
       "\n",
       "            # sig BM FWD BD  w/ no ns fwd bd  prop ns pos fwd bd     ...      \\\n",
       "Region                                                               ...       \n",
       "acc                      13                3            0.950000     ...       \n",
       "angular                   4                1            1.000000     ...       \n",
       "cerebellum                3                1            1.000000     ...       \n",
       "dlpfc                     2                0            1.000000     ...       \n",
       "fusiform                  7                0            0.916667     ...       \n",
       "\n",
       "            # sig BM REV PC  w/ no ns rev pc  prop pos rev pc  mean r fwd bd  \\\n",
       "Region                                                                         \n",
       "acc                       5                0            0.875       0.157133   \n",
       "angular                   4                1                1       0.166280   \n",
       "cerebellum                6                5                1       0.102333   \n",
       "dlpfc                    10                3      0.263157895       0.322467   \n",
       "fusiform                  6                3      0.714285714       0.173717   \n",
       "\n",
       "            mean r rev bd  mean r fwd pc  mean r rev pc  mean of means  \\\n",
       "Region                                                                   \n",
       "acc              0.173140       0.131200       0.186700       0.162043   \n",
       "angular          0.166280       0.246729       0.216888       0.199044   \n",
       "cerebellum       0.102333       0.150200       0.150200       0.126267   \n",
       "dlpfc            0.322467       0.000000      -0.047241       0.149423   \n",
       "fusiform         0.187000       0.312700       0.173671       0.211772   \n",
       "\n",
       "           avg func ns top50  difference  \n",
       "Region                                    \n",
       "acc                 0.417895    0.231195  \n",
       "angular             0.354437    0.137550  \n",
       "cerebellum          0.243906    0.093706  \n",
       "dlpfc               0.422167    0.469408  \n",
       "fusiform            0.388636    0.214964  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pd.read_csv('/Users/Katie/Dropbox/Data/NSvBM-decoding/stats.csv', index_col=0, header=0)\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pointbiserialr, pearsonr\n",
    "\n",
    "corrs = {}\n",
    "corrs['FPxInfAll'] = spearmanr(stats['Mean FP'], stats['Informative NS terms - pos'])\n",
    "corrs['FPxInf50'] = spearmanr(stats['Mean FP'], stats['Informative NS terms top 50'])\n",
    "corrs['FPxInf20'] = spearmanr(stats['Mean FP'], stats['Informative NS terms top 20'])\n",
    "corrs['SizexInfAll'] = spearmanr(stats['Size (mm)'], stats['Informative NS terms - pos'])\n",
    "corrs['SizexInf50'] = spearmanr(stats['Size (mm)'], stats['Informative NS terms top 50'])\n",
    "corrs['SizexInf20'] = spearmanr(stats['Size (mm)'], stats['Informative NS terms top 20'])\n",
    "corrs['AnatxInfAll'] = pointbiserialr(stats['Anat?'], stats['Informative NS terms - pos'])\n",
    "corrs['AnatxInf50'] = pointbiserialr(stats['Anat?'], stats['Informative NS terms top 50'])\n",
    "corrs['AnatxInf20'] = pointbiserialr(stats['Anat?'], stats['Informative NS terms top 20'])\n",
    "\n",
    "corrs['FPxFwdBd'] = spearmanr(stats['Mean FP'], stats['# sig BM FWD BD'])\n",
    "corrs['FPxRevBd'] = spearmanr(stats['Mean FP'], stats['# sig BM REV BD'])\n",
    "corrs['FPxFwdPc'] = spearmanr(stats['Mean FP'], stats['# sig BM FWD PC'])\n",
    "corrs['FPxRevPc'] = spearmanr(stats['Mean FP'], stats['# sig BM REV PC'])\n",
    "corrs['SizexFwdBd'] = spearmanr(stats['Size (mm)'], stats['# sig BM FWD BD'])\n",
    "corrs['SizexRevBd'] = spearmanr(stats['Size (mm)'], stats['# sig BM REV BD'])\n",
    "corrs['SizexFwdPc'] = spearmanr(stats['Size (mm)'], stats['# sig BM FWD PC'])\n",
    "corrs['SizexRevPc'] = spearmanr(stats['Mean FP'], stats['# sig BM REV PC'])\n",
    "corrs['AnatxFwdBd'] = pointbiserialr(stats['Anat?'], stats['# sig BM FWD BD'])\n",
    "corrs['AnatxRevBd'] = pointbiserialr(stats['Anat?'], stats['# sig BM REV BD'])\n",
    "corrs['AnatxFwdPc'] = pointbiserialr(stats['Anat?'], stats['# sig BM FWD PC'])\n",
    "corrs['AnatxRevPc'] = pointbiserialr(stats['Anat?'], stats['# sig BM REV PC'])\n",
    "\n",
    "corrs['FPxFwdBdPosNs'] = spearmanr(stats['Mean FP'], stats['prop ns pos fwd bd'])\n",
    "corrs['FPxRevBdPosNs'] = spearmanr(stats['Mean FP'], stats['prop pos rev bd'])\n",
    "corrs['FPxFwdPcPosNs'] = spearmanr(stats['Mean FP'], stats['prop pos fwd pc'])\n",
    "#corrs['FPxRevPcPosNs'] = pearsonr(stats['Mean FP'], stats['prop pos rev pc'])\n",
    "\n",
    "corrs['SizexFwdBdPosNs'] = spearmanr(stats['Size (mm)'], stats['prop ns pos fwd bd'])\n",
    "corrs['SizexRevBdPosNs'] = spearmanr(stats['Size (mm)'], stats['prop pos rev bd'])\n",
    "corrs['SizexFwdPcPosNs'] = spearmanr(stats['Size (mm)'], stats['prop pos fwd pc'])\n",
    "#corrs['SizexRevPcPosNs'] = pearsonr(stats['Size (mm)'], stats['prop pos rev pc'])\n",
    "\n",
    "corrs['AnatxFwdBdPosNs'] = pointbiserialr(stats['Anat?'], stats['prop ns pos fwd bd'])\n",
    "corrs['AnatxRevBdPosNs'] = pointbiserialr(stats['Anat?'], stats['prop pos rev bd'])\n",
    "corrs['AnatxFwdPcPosNs'] = pointbiserialr(stats['Anat?'], stats['prop pos fwd pc'])\n",
    "#corrs['AnatxRevPcPosNs'] = pointbiserialr(stats['Anat?'], stats['prop pos rev pc'])\n",
    "\n",
    "corrs['FPxbm_r'] = spearmanr(stats['Mean FP'], stats['mean of means'])\n",
    "corrs['FPxns_r'] = spearmanr(stats['Mean FP'], stats['avg func ns top50'])\n",
    "corrs['FPxdiff'] = spearmanr(stats['Mean FP'], stats['difference'])\n",
    "#corrs['FPxRevPcPosNs'] = pearsonr(stats['Mean FP'], stats['prop pos rev pc'])\n",
    "\n",
    "corrs['Sizexbm_r'] = spearmanr(stats['Size (mm)'], stats['mean of means'])\n",
    "corrs['Sizexns_r'] = spearmanr(stats['Size (mm)'], stats['avg func ns top50'])\n",
    "corrs['Sizexdiff'] = spearmanr(stats['Size (mm)'], stats['difference'])\n",
    "#corrs['SizexRevPcPosNs'] = pearsonr(stats['Size (mm)'], stats['prop pos rev pc'])\n",
    "\n",
    "corrs['Anatxbm_r'] = pointbiserialr(stats['Anat?'], stats['mean of means'])\n",
    "corrs['Anatxns_r'] = pointbiserialr(stats['Anat?'], stats['avg func ns top50'])\n",
    "corrs['Anatxdiff'] = pointbiserialr(stats['Anat?'], stats['difference'])\n",
    "#corrs['AnatxRevPcPosNs'] = pointbiserialr(stats['Anat?'], stats['prop pos rev pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AnatxFwdBd': PointbiserialrResult(correlation=0.14711105167260857, pvalue=0.60084131601877655),\n",
       " 'AnatxFwdBdPosNs': PointbiserialrResult(correlation=0.35800619256252592, pvalue=0.19012668092171778),\n",
       " 'AnatxFwdPc': PointbiserialrResult(correlation=0.08821135213411202, pvalue=0.75457540757669628),\n",
       " 'AnatxFwdPcPosNs': PointbiserialrResult(correlation=-0.18199417769731444, pvalue=0.51623025707487558),\n",
       " 'AnatxInf20': PointbiserialrResult(correlation=0.35773338974388708, pvalue=0.19048897706735116),\n",
       " 'AnatxInf50': PointbiserialrResult(correlation=0.44843497052615244, pvalue=0.09363592226154889),\n",
       " 'AnatxInfAll': PointbiserialrResult(correlation=-0.083752416913701486, pvalue=0.7666555249935405),\n",
       " 'AnatxRevBd': PointbiserialrResult(correlation=0.25890932810266104, pvalue=0.35144761020705589),\n",
       " 'AnatxRevBdPosNs': PointbiserialrResult(correlation=0.35095918148079958, pvalue=0.19962887316964478),\n",
       " 'AnatxRevPc': PointbiserialrResult(correlation=-0.3692561082073017, pvalue=0.17557342420496705),\n",
       " 'Anatxbm_r': PointbiserialrResult(correlation=0.44827938974636594, pvalue=0.093763701150280584),\n",
       " 'Anatxdiff': PointbiserialrResult(correlation=-0.15338033013948177, pvalue=0.58523712464988398),\n",
       " 'Anatxns_r': PointbiserialrResult(correlation=0.28458763443013724, pvalue=0.30393062252316033),\n",
       " 'FPxFwdBd': SpearmanrResult(correlation=-0.23488521226487544, pvalue=0.3994133973911671),\n",
       " 'FPxFwdBdPosNs': SpearmanrResult(correlation=0.016957604552904227, pvalue=0.9521696372930073),\n",
       " 'FPxFwdPc': SpearmanrResult(correlation=-0.25524063793062324, pvalue=0.35855584901530446),\n",
       " 'FPxFwdPcPosNs': SpearmanrResult(correlation=-0.099861449033769353, pvalue=0.72326732776537717),\n",
       " 'FPxInf20': SpearmanrResult(correlation=-0.1711781179121562, pvalue=0.54187217790321562),\n",
       " 'FPxInf50': SpearmanrResult(correlation=-0.34924638299969801, pvalue=0.20198358542949654),\n",
       " 'FPxInfAll': SpearmanrResult(correlation=-0.18945494607122559, pvalue=0.49887235627012882),\n",
       " 'FPxRevBd': SpearmanrResult(correlation=-0.1393793582442793, pvalue=0.62030882997419357),\n",
       " 'FPxRevBdPosNs': SpearmanrResult(correlation=-0.31246314975473272, pvalue=0.25685306611133157),\n",
       " 'FPxRevPc': SpearmanrResult(correlation=0.047063159926639765, pvalue=0.86772208916667914),\n",
       " 'FPxbm_r': SpearmanrResult(correlation=-0.34999999999999998, pvalue=0.20094535081629331),\n",
       " 'FPxdiff': SpearmanrResult(correlation=0.035714285714285705, pvalue=0.89944699347208745),\n",
       " 'FPxns_r': SpearmanrResult(correlation=-0.31428571428571428, pvalue=0.25394001632367141),\n",
       " 'SizexFwdBd': SpearmanrResult(correlation=0.15719241128495509, pvalue=0.57583072852585404),\n",
       " 'SizexFwdBdPosNs': SpearmanrResult(correlation=-0.10174562731742537, pvalue=0.718240503769357),\n",
       " 'SizexFwdPc': SpearmanrResult(correlation=0.038840966641616584, pvalue=0.89069054181191698),\n",
       " 'SizexFwdPcPosNs': SpearmanrResult(correlation=0.0037683565673120507, pvalue=0.98936570306029759),\n",
       " 'SizexInf20': SpearmanrResult(correlation=-0.11712187015042266, pvalue=0.67763380093060288),\n",
       " 'SizexInf50': SpearmanrResult(correlation=-0.40325355562851734, pvalue=0.13610571037044644),\n",
       " 'SizexInfAll': SpearmanrResult(correlation=-0.21090267581513797, pvalue=0.4505438231893526),\n",
       " 'SizexRevBd': SpearmanrResult(correlation=0.095936441388919533, pvalue=0.73377248003286843),\n",
       " 'SizexRevBdPosNs': SpearmanrResult(correlation=-0.25222928956104929, pvalue=0.36444933075516572),\n",
       " 'SizexRevPc': SpearmanrResult(correlation=0.047063159926639765, pvalue=0.86772208916667914),\n",
       " 'Sizexbm_r': SpearmanrResult(correlation=-0.21071428571428572, pvalue=0.45095786521528503),\n",
       " 'Sizexdiff': SpearmanrResult(correlation=-0.096428571428571405, pvalue=0.73245286338673077),\n",
       " 'Sizexns_r': SpearmanrResult(correlation=-0.067857142857142852, pvalue=0.8101088187105131)}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73333333333333328"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(stats['w/ no ns fwd bd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.18445751397544621, 0.51046891142243167)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(stats['Informative NS terms - pos'], stats['w/ no ns rev pc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
